{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - \"The Office\" dataset\n",
    "This notebook aims to provide parameterizable functions to preprocess the \"The Office\" dataset for further NLP analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "PATH = \"../data/\"\n",
    "FILE = \"unmod_the-office-lines - scripts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH+FILE, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate line_text for each scene\n",
    "def concatenate_scenes(df):\n",
    "    df = df.groupby([\"season\", \"episode\", \"scene\"])[\"line_text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_directorals(df):\n",
    "    # extract text from line_text in square brackets, put it in new column called \"directionals\", multiple square brackets will be extracted as a list\n",
    "    df[\"directionals\"] = df[\"line_text\"].str.extractall(r\"\\[(.*?)\\]\").unstack().apply(lambda x: \", \".join(x.dropna()), axis=1)\n",
    "    # delete the extracted text from line_text\n",
    "    df[\"line_text\"] = df[\"line_text\"].str.replace(r\"\\[(.*?)\\]\", \"\", regex=True).str.strip()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare string preprocessing\n",
    "def remove_punctuation(df):\n",
    "    return df[\"line_text\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "def lower(df):\n",
    "    return df[\"line_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "def expanding_contractions(df):\n",
    "    return df[\"line_text\"].apply(lambda x: contractions.fix(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    return df[\"line_text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# TODO: make this paremeterizable\n",
    "def segmentation(df):\n",
    "    return df[\"line_text\"].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "def lemmatize(df):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # is working, but not very good results because of the simple speech of the characters\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "\n",
    "def stem(df):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return df[\"line_text\"].apply(lambda x: \" \".join([porter_stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# tagging\n",
    "def pos_tag(df):\n",
    "    return df[\"line_text\"].apply(lambda x: nltk.pos_tag(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, concat_scenes=False, extract_direc=False, remove_punct=False, rmv_stopwords=False, lwr=False, exp_contractions=False, conversion=None):\n",
    "    if (concat_scenes):\n",
    "        df = concatenate_scenes(df)\n",
    "    if (extract_direc):\n",
    "        df = extract_directorals(df)\n",
    "\n",
    "    if (remove_punct):\n",
    "        df['line_text'] = remove_punctuation(df)\n",
    "    if (lwr):\n",
    "        df['line_text'] = lower(df)\n",
    "    if (rmv_stopwords):\n",
    "        df['line_text'] = remove_stopwords(df)\n",
    "    if (exp_contractions):\n",
    "        df['line_text'] = expanding_contractions(df)   \n",
    "\n",
    "    if (conversion == \"tokenize\"):\n",
    "        df['line_text']  = tokenize(df)\n",
    "    elif (conversion == \"segment\"):\n",
    "        df['line_text'] = segmentation(df)\n",
    "    elif (conversion == \"lemmatize\"):\n",
    "        df['line_text'] = lemmatize(df)\n",
    "    elif (conversion == \"stem\"):\n",
    "        df['line_text'] = stem(df)\n",
    "    elif (conversion == \"pos_tag\"):\n",
    "        df['line_text'] = pos_tag(df)\n",
    "\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param_dict = {\n",
    "    \"concat_scenes\": True,\n",
    "    \"extract_direc\": True, \n",
    "    \"remove_punct\": True, \n",
    "    \"rmv_stopwords\": True,\n",
    "    \"lwr\": True, \n",
    "    \"exp_contractions\": True,\n",
    "    \"conversion\": None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>line_text</th>\n",
       "      <th>directionals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>right jim quarterlies look good things library oh told could not close you have come master guidance you are saying grasshopper actually called yeah right well let show done</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes id like speak office manager please yes hello michael scott regional manager dunder mifflin paper products wanted talk manageramanger right done deal thank much sir you are gentleman scholar oh i am sorry ok i am sorry mistake woman talking low voice probably smoker that is way done</td>\n",
       "      <td>on the phone, quick cut scene, hangs up, Clears throat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>i have uh i have dunder mifflin 12 years last four regional manager want come see entire floor kingdom far eye see receptionist pam pam pampam pam beesly pam us forever right pam well do not know think she is cute seen couple years ago messages uh yeah fax oh pam corporate many times told there is special filing cabinet things corporate have not told called wastepaper basket look look face</td>\n",
       "      <td>growls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>people say best boss go god we have never worked place like you are hilarious get best us think pretty much sums found spencer gifts</td>\n",
       "      <td>shows the camera his WORLD'S BEST BOSS mug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>shall play pa rum pump um pum gifts pa rum pump um pum</td>\n",
       "      <td>singing, Imitates heavy drumming, Imitates heavy drumming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>seems arbitrary applied job company hiring took desk back empty matter get end human beings miraculous gift make place home let</td>\n",
       "      <td>chuckles, standing with two cops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>113</td>\n",
       "      <td>feel lucky got chance share crummy story anyone thinks one take dump paper shredder alone sister let get beer sometime</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9158</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>114</td>\n",
       "      <td>happy filmed remember everyone worked paper company years never wrote anything</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>115</td>\n",
       "      <td>sold paper company 12 years job speak clients phone quantities types copier paper even love every minute everything owe job stupid wonderful boring amazing job</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9160</th>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>thought weird picked us make documentary think ordinary paper company like dunder mifflin great subject documentary lot beauty ordinary things kind point</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9161 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  episode  scene  \\\n",
       "0          1        1      1   \n",
       "1          1        1      2   \n",
       "2          1        1      3   \n",
       "3          1        1      4   \n",
       "4          1        1      5   \n",
       "...      ...      ...    ...   \n",
       "9156       9       23    112   \n",
       "9157       9       23    113   \n",
       "9158       9       23    114   \n",
       "9159       9       23    115   \n",
       "9160       9       23    116   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                     line_text  \\\n",
       "0                                                                                                                                                                                                                                right jim quarterlies look good things library oh told could not close you have come master guidance you are saying grasshopper actually called yeah right well let show done   \n",
       "1                                                                                                              yes id like speak office manager please yes hello michael scott regional manager dunder mifflin paper products wanted talk manageramanger right done deal thank much sir you are gentleman scholar oh i am sorry ok i am sorry mistake woman talking low voice probably smoker that is way done   \n",
       "2     i have uh i have dunder mifflin 12 years last four regional manager want come see entire floor kingdom far eye see receptionist pam pam pampam pam beesly pam us forever right pam well do not know think she is cute seen couple years ago messages uh yeah fax oh pam corporate many times told there is special filing cabinet things corporate have not told called wastepaper basket look look face   \n",
       "3                                                                                                                                                                                                                                                                         people say best boss go god we have never worked place like you are hilarious get best us think pretty much sums found spencer gifts   \n",
       "4                                                                                                                                                                                                                                                                                                                                                       shall play pa rum pump um pum gifts pa rum pump um pum   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "9156                                                                                                                                                                                                                                                                           seems arbitrary applied job company hiring took desk back empty matter get end human beings miraculous gift make place home let   \n",
       "9157                                                                                                                                                                                                                                                                                    feel lucky got chance share crummy story anyone thinks one take dump paper shredder alone sister let get beer sometime   \n",
       "9158                                                                                                                                                                                                                                                                                                                            happy filmed remember everyone worked paper company years never wrote anything   \n",
       "9159                                                                                                                                                                                                                                           sold paper company 12 years job speak clients phone quantities types copier paper even love every minute everything owe job stupid wonderful boring amazing job   \n",
       "9160                                                                                                                                                                                                                                                 thought weird picked us make documentary think ordinary paper company like dunder mifflin great subject documentary lot beauty ordinary things kind point   \n",
       "\n",
       "                                                   directionals  \n",
       "0                                                           NaN  \n",
       "1        on the phone, quick cut scene, hangs up, Clears throat  \n",
       "2                                                        growls  \n",
       "3                    shows the camera his WORLD'S BEST BOSS mug  \n",
       "4     singing, Imitates heavy drumming, Imitates heavy drumming  \n",
       "...                                                         ...  \n",
       "9156                           chuckles, standing with two cops  \n",
       "9157                                                        NaN  \n",
       "9158                                                        NaN  \n",
       "9159                                                        NaN  \n",
       "9160                                                        NaN  \n",
       "\n",
       "[9161 rows x 5 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess(df, **param_dict)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "def extract_features(df, vectorizer):\n",
    "    if vectorizer == \"binary\":\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "    elif vectorizer == \"count\":\n",
    "        vectorizer = CountVectorizer() \n",
    "    elif vectorizer == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif vectorizer == \"hashing\":\n",
    "        vectorizer = HashingVectorizer()\n",
    "\n",
    "    result = vectorizer.fit_transform(df[\"line_text\"])\n",
    "    return result\n",
    "\n",
    "def feature_selection (feature_df, selection_method):\n",
    "    # TODO: add feature selection e.g. DF (document frequency)\n",
    "    print(\"nothin here yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "feature_df = extract_features(df, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data\n",
    "df.to_csv(PATH+\"preprocessed_\"+FILE, sep=\",\", index=True)\n",
    "feature_df.to_csv(PATH+\"feature_\"+FILE, sep=\",\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d08bd1c3f8bce5cbee9b581e4183f55ee085546009e4ad69f297b57f92833616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
